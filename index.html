<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Dejavu: Post‑Deployment Learning for Embodied Agents via Experience Feedback</title>
  <meta name="description" content="ICLR 2026 under review. Dejavu (EFN): a post-deployment learning framework that augments frozen VLA policies with an Experience Feedback Network.">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap" rel="stylesheet">
  <!-- Bulma (used by nerfies.github.io template) -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <!-- Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.4/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
  <style>
    :root { --brand:#4F46E5; --soft:#EEF2FF; }
    html, body { font-family: Inter, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol"; }
    .hero.is-light { background: linear-gradient(180deg, var(--soft), white); }
    .publication-title { font-weight: 800; letter-spacing: -0.02em; }
    .author-block a { color:#363636; text-decoration: underline dotted #bdbdbd; }
    .teaser { border-radius: 12px; box-shadow: 0 6px 32px rgba(0,0,0,0.08); }
    .btn-row a.button { margin: 6px 8px 0 0; }
    .section-title { font-weight: 700; font-size: 1.6rem; letter-spacing:-0.01em; }
    .figure { text-align:center; }
    .figure img { border-radius: 12px; box-shadow: 0 6px 28px rgba(0,0,0,0.08); }
    code.bibtex { white-space: pre; display:block; padding:1rem; background:#0f172a; color:#e5e7eb; border-radius:12px; overflow:auto; }
    .tag.is-venue { background: #E0E7FF; color:#1e1b4b; border-radius:999px; }
    .footer { background: #fafafa; }
  </style>
</head>
<body>
  <!-- Hero -->
  <section class="hero is-light">
    <div class="hero-body">
      <div class="container">
        <div class="has-text-centered">
          <h1 class="title is-2 publication-title">Dejavu: Post‑Deployment Learning for Embodied Agents<br>via Experience Feedback</h1>
          <p class="subtitle is-5">
            <span class="tag is-venue is-medium">ICLR 2026 — under review</span>
          </p>
          <p class="author-block">
            <em>Anonymous authors</em>
          </p>
          <p class="btn-row">
            <a class="button is-link is-light" href="assets/dejavu.pdf" target="_blank"><span class="icon"><i class="ai ai-arxiv"></i></span><span>Paper (PDF)</span></a>
            <a class="button is-link is-light" href="assets/dejavu_efn.zip" target="_blank"><span class="icon"><i class="fa-solid fa-globe"></i></span><span>Code</span></a>
            <a class="button is-link is-light" href="#bibtex"><span class="icon"><i class="fa-solid fa-quote-right"></i></span><span>BibTeX</span></a>
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser -->
  <section class="section">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-10">
          <figure class="image is-16by9 teaser">
            <!-- Replace with your real teaser image or video poster -->
            <img src="assets/teaser.png" alt="EFN augments a frozen VLA with retrieved experience trajectories and residual action corrections.">
          </figure>
          <p class="has-text-grey is-size-6 has-text-centered" style="margin-top:.5rem;">
            Figure 1. A frozen VLA is augmented by an <strong>Experience Feedback Network (EFN)</strong> that retrieves task‑relevant trajectories,
            predicts a residual correction, and steers the next observation toward the retrieved successor frame.
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- Abstract -->
  <section class="section">
    <div class="container">
      <h2 class="section-title">Abstract</h2>
      <div class="content is-size-5">
        <p>
          We present <strong>Dejavu</strong>, a general post‑deployment learning framework that augments a frozen Vision‑Language‑Action (VLA) policy
          with an <em>Experience Feedback Network (EFN)</em> and a live <em>experience bank</em>. EFN retrieves a contextually successful prior step
          and conditions action prediction on this guidance by outputting a residual correction that refines the base policy’s action.
          EFN is optimized with reinforcement learning using a dense, similarity‑based reward that compares the realized next observation
          to the next observation in the retrieved trajectory. During deployment, the agent continually appends successful rollouts to its
          memory, enabling measurable improvement without modifying the backbone weights.
        </p>
      </div>
      <div class="buttons btn-row">
        <a class="button is-link" href="assets/dejavu.pdf" target="_blank"><span class="icon"><i class="fa-solid fa-file-pdf"></i></span><span>Read the paper</span></a>
      </div>
    </div>
  </section>

  <!-- Links -->
  <!-- <section class="section">
    <div class="container">
      <h2 class="section-title">Resources</h2>
      <div class="buttons btn-row">
        <a class="button is-primary" href="assets/dejavu.pdf" target="_blank"><span class="icon"><i class="fa-solid fa-file-pdf"></i></span><span>Paper (PDF)</span></a>
        <a class="button is-primary is-light" href="https://dejavu2025.github.io/" target="_blank"><span class="icon"><i class="fa-solid fa-globe"></i></span><span>Demo</span></a>
        <a class="button is-primary is-light" href="assets/poster.pdf" target="_blank"><span class="icon"><i class="fa-solid fa-image"></i></span><span>Poster</span></a>
        <a class="button is-primary is-light" href="assets/slides.pdf" target="_blank"><span class="icon"><i class="fa-solid fa-person-chalkboard"></i></span><span>Slides</span></a>
        <a class="button is-primary is-light" href="#bibtex"><span class="icon"><i class="fa-solid fa-quote-right"></i></span><span>BibTeX</span></a>
      </div>
    </div>
  </section> -->

  <section class="section">
    <div class="container">
      <h2 class="section-title">Paper</h2>
        <div class="content has-text-centered">
          <iframe src="assets/dejavu.pdf#view=FitH" style="width:100%; height:600px; border:1px solid #eee; border-radius: 8px;"></iframe>
          <p class="has-text-grey is-size-7" style="margin-top: .5rem;">If the preview does not load, use the PDF button above.</p>
        </div>
    </div>
  </section>


  <!-- Method -->
  <section class="section">
    <div class="container">
      <h2 class="section-title">Method Overview</h2>
      <div class="content">
        <p>
          An <strong>experience</strong> is defined as a synchronized trajectory of vision, language, and action. At each step, EFN retrieves a
          task‑filtered candidate from the bank using language‑conditioned visual similarity, then predicts a <em>residual action</em> that is
          added to the base VLA output. A dense semantic reward encourages the next observation to resemble the successor frame from
          the retrieved experience while regularizing residual magnitude and discouraging idling. We train EFN with <em>Soft Actor–Critic</em>
          (SAC) and deploy with deterministic residual corrections.
        </p>
      </div>
      <div class="columns is-multiline">
        <div class="column is-6">
          <div class="box figure">
            <img src="assets/train.png" alt="Training pipeline: retrieval + residual policy learning with similarity rewards.">
            <p class="has-text-grey is-size-7" style="margin-top:.5rem;">Training: retrieve, correct, and optimize with dense similarity rewards.</p>
          </div>
        </div>
        <div class="column is-6">
          <div class="box figure">
            <img src="assets/test.png" alt="Inference pipeline: instruction‑filtered retrieval and online experience growth.">
            <p class="has-text-grey is-size-7" style="margin-top:.5rem;">Inference: instruction‑filtered retrieval and online experience growth.</p>
          </div>
        </div>
      </div>
    </div>
  </section>



  <!-- Results -->
  <section class="section">
    <div class="container">
      <h2 class="section-title">Results</h2>
      <div class="content">
        <p>
          On <em>LIBERO</em> simulation and a real‑world <em>AgiBot‑G1</em> platform, EFN consistently increases success rate and reduces steps across
          VLA backbones (OpenVLA, UniVLA), with further gains from larger experience banks. Improvements are most pronounced on
          long‑horizon tasks, indicating recall‑guided residuals effectively truncate redundant behavior.
        </p>
      </div>
        <div class="box figure">
            <img src="assets/tab1.png" alt="LIBERO results">
            <p class="has-text-grey is-size-7" style="margin-top:.5rem;">LIBERO: Deployment performance with and without EFN.</p>
        </div>
        <div class="box figure">
            <img src="assets/tab2.png" alt="Real robot results">
            <p class="has-text-grey is-size-7" style="margin-top:.5rem;">Real‑world: AgiBot‑G1 with GO‑1 across three tasks.</p>
        </div>
    </div>
  </section>

  <!-- Paper teaser / video (optional) -->
  <section class="section">
    <div class="container">
      <h2 class="section-title">Demos</h2>
      <div class="columns is-centered">
        <div class="column is-10">
          <div class="box">
            <video class="teaser" controls playsinline>
              <source src="assets/putbottle.mp4" type="video/mp4">
            </video>
            <video class="teaser" controls playsinline>
              <source src="assets/sortitem.mp4" type="video/mp4">
            </video>
            <video class="teaser" controls playsinline>
              <source src="assets/addgoods.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Citation -->
  <section class="section" id="bibtex">
    <div class="container">
      <h2 class="section-title">BibTeX</h2>
      <code class="bibtex">@inproceedings{dejavu_iclr2026,
  title     = {Dejavu: Post-Deployment Learning for Embodied Agents via Experience Feedback},
  author    = {Anonymous},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2026},
  note      = {under double-blind review},
  url       = {https://dejavu2025.github.io/}
}</code>
    </div>
  </section>

  <!-- Acknowledgements -->
  <section class="section">
    <div class="container">
      <h2 class="section-title">Acknowledgements</h2>
      <p class="content">
        This website follows the structure of the <em>nerfies.github.io</em> academic project template.
      </p>
    </div>
  </section>

  <footer class="footer">
    <div class="content has-text-centered">
      <p>
        <strong>Dejavu (EFN)</strong> — ICLR 2026 (under review). Site template inspired by <em>nerfies.github.io</em>.
      </p>
    </div>
  </footer>
</body>
</html>
